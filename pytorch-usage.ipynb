{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python2717jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3fa8e34f0289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdataset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import webdataset as wds\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms"
   ]
  },
  {
   "source": [
    "### Usage with PyTorch\n",
    "\n",
    "Below we provide a simple DataModule class which can be used with PyTorch-Lightning for loading the sharded TARs. This setup will work with DDP modes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ataModule(pl.LightningDataModule):\n",
    "    def __init__(self, cfg, train_augs=[transforms.Resize(800, 640)])\n",
    "    )], val_augs=[]):\n",
    "        super().__init__()\n",
    "        self.total_batch_size = cfg.optim.batch_size\n",
    "        self.world_size = cfg.compute.world_size\n",
    "        self.cfg = cfg\n",
    "        self.train_augmentation = train_augs\n",
    "        self.valid_augmentation = val_augs\n",
    "        assert self.total_batch_size % cfg.compute.world_size == 0\n",
    "        self.local_batch_size = self.total_batch_size // cfg.compute.world_size\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_anns(anns: List[Any]):\n",
    "        # convert the pose anns to d2 ann\n",
    "        data = [{\n",
    "            \"bbox\": a[\"bbox_2d\"],\n",
    "            \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "            \"keypoints\": [],\n",
    "            \"category_id\": a[\"category_id\"] if \"category_id\" in a else 0,\n",
    "            \"keypoints\": a[\"bbox_3d\"],\n",
    "            \"translation\": a[\"cam_t_m2c\"],\n",
    "            \"rotation\": a[\"cam_R_m2c\"],\n",
    "            \"bbox_3d\": a[\"bbox_3d\"]\n",
    "        } for a in anns]\n",
    "        return data\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        batch_sampler = torch.utils.data.sampler.BatchSampler(\n",
    "            self.train_sampler, self.local_batch_size, drop_last=True\n",
    "        )  # drop_last so the batch always have the same size\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            num_workers=self.cfg.dataloader.num_workers,\n",
    "            batch_sampler=batch_sampler,\n",
    "            collate_fn=trivial_batch_collator,\n",
    "            worker_init_fn=worker_init_reset_seed,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        batch_sampler = torch.utils.data.sampler.BatchSampler(\n",
    "            self.valid_sampler, self.local_batch_size, drop_last=False\n",
    "        )  # drop_last so the batch always have the same size\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            num_workers=self.cfg.dataloader.num_workers,\n",
    "            batch_sampler=batch_sampler,\n",
    "            collate_fn=trivial_batch_collator,\n",
    "            worker_init_fn=worker_init_reset_seed,\n",
    "        )\n"
   ]
  }
 ]
}